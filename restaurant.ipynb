{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import happybase\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializition for connecting to hbase using thrift server\n",
    "batch_size = 1\n",
    "host = \"0.0.0.0\"\n",
    "file_path = \"/home/user1/Downloads/rest.csv\"\n",
    "namespace = \"default\"\n",
    "row_count = 0\n",
    "start_time = time.time()\n",
    "table_name = \"REST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connect_to_hbase():\n",
    "    \"\"\" Connect to HBase server.\n",
    "    This will use the host, namespace, table name, and batch size as defined in\n",
    "    the global variables above.\n",
    "    \"\"\"\n",
    "    conn = happybase.Connection(host = host,\n",
    "        table_prefix = namespace,\n",
    "        table_prefix_separator = \":\",autoconnect=True )\n",
    "    #conn.open()\n",
    "    table = conn.table(table_name)\n",
    "    batch = table.batch(batch_size = batch_size)\n",
    "    return conn,batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_row(batch, row):\n",
    "    \"\"\" Insert a row into HBase.\n",
    "    Write the row to the batch. When the batch size is reached, rows will be\n",
    "    sent to the database.\n",
    "    Rows have the following schema:\n",
    "        [ id\ttyp\ttime\tintent\tentity\tcity\tstate\tcountry\trestname\tpaytype\tyear\tmonth\tdate]\n",
    "    \"\"\"\n",
    "    batch.put(row[0], { \"data:typ\": row[1], \"data:time\": row[2], \"data:intent\": row[3],\n",
    "        \"data:entity\": row[4], \"data:city\": row[5], \"data:state\": row[6],\"data:country\": row[7],\"data:restname\": row[8], \n",
    "                  \"data:paytype\": row[9],\"data:year\": row[10],\"data:month\": row[11],\"data:date\": row[12]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_csv():\n",
    "    csvfile = open(file_path, \"r\")\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    return csvreader, csvfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn, batch = connect_to_hbase()\n",
    "print (\"Connect to HBase. table name: %s, batch size: %i\" % (table_name, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvreader, csvfile = read_csv()\n",
    "print (\"Connected to file. name: %s\" % (file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Loop through the rows. The first row contains column headers, so skip that\n",
    "    # row. Insert all remaining rows into the database.\n",
    "    for row in csvreader:\n",
    "        row_count += 1\n",
    "        if row_count == 1:\n",
    "            pass\n",
    "        else:\n",
    "            insert_row(batch, row)\n",
    "\n",
    "    # If there are any leftover rows in the batch, send them now.\n",
    "    batch.send()\n",
    "finally:\n",
    "    # No matter what happens, close the file handle.\n",
    "    csvfile.close()\n",
    "    #conn.close()\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print (\"Done. row count: %i, duration: %.3f s\" % (row_count, duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading from Hbase using the connection \n",
    "conn, batch = connect_to_hbase()\n",
    "table = conn.table('REST')\n",
    "rows = table.scan(columns=[b'data'])\n",
    "#Intialize df as dataframe\n",
    "df = pd.DataFrame()\n",
    "print(df)\n",
    "for row in rows:\n",
    "    df_row = {key.decode('utf8').split(':')[1]: value.decode('utf8') for key, value in row[1].items()}\n",
    "    df = df.append(df_row, ignore_index=True)\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Groupby the desired columns and convert it to csv to do some preprocessing before converting to json\n",
    "df1=df.groupby(['typ','intent','month']).size()\n",
    "df3=df1.to_csv()\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Same preprocessing to rename it as tp convert it to json\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "TESTDATA = StringIO(df3)\n",
    "df4 = pd.read_csv(TESTDATA, sep=\",\",header=None)\n",
    "df4.rename(columns={0: 'typ', 1: 'intent',2: 'month',3: 'count'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/19798112/convert-pandas-dataframe-to-a-nested-dict\n",
    "#This is recursive function to convert pandas dataframe to nested dictionary of any level\n",
    "def recur_dictify(frame):\n",
    "    if len(frame.columns) == 1:\n",
    "        if frame.values.size == 1: return frame.values[0][0]\n",
    "        return frame.values.squeeze()\n",
    "    grouped = frame.groupby(frame.columns[0])\n",
    "    d = {k: recur_dictify(g.ix[:,1:]) for k,g in grouped}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j1=recur_dictify(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to convert numpy int64 data type to int data type. This is needed as json dumps can \n",
    "#work only on integer data types\n",
    "import numpy\n",
    "def default(o):\n",
    "    if isinstance(o, numpy.int64): return int(o)  \n",
    "    raise TypeError\n",
    "\n",
    "x6=json.dumps(j1, default=default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
